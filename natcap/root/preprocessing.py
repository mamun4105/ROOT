"""Spatial processing functions.

The :meth:`execute` function in this module performs the entire workflow of converting
spatial inputs to ROOT to the input tables required for the optimization step.
"""
import os
import sys
import shutil
import math
import tempfile
import collections
import glob
import shapely.wkb
import shapely.prepared

import numpy
from osgeo import ogr
from osgeo import gdal
from osgeo import osr
import numpy as np
import pandas as pd

import pygeoprocessing
from natcap.root import arith_parser as ap


class RootPreprocessingError(Exception):
    pass


def execute(args):
    """Encapsulate optimization workflow.

    This algorithm creates an SDU grid, a marginal gain raster,
    aggregate those value to the grid, create a table for an IP
    optimizer, and invokes that optimizer.

    Parameters:
        args (dict): must contain the following fields:

            * activity_masks: *tbd*
            * baseline_file: *tbd*
            * cell_size: *tbd*
            * combined_factors: *tbd*
            * csv_output_folder: *tbd*
            * grid_type: *tbd**
            * mask_raster: *tbd*
            * raster_li: *tbd*
            * raster_table: *tbd*
            * sdu_id_col: *tbd*
            * serviceshed_list: *tbd*
            * workspace: *tbd*

            raster_list (list): list of paths to single band raster paths on
                disk that represent per-pixel marginal values.  These rasters
                must be the same size and in geospatial projection.
            value_names (list): a parallel list of `raster_list` representing
                the well known names of the raster paths in `raster_list`.
                These names are later used by the optimizer for objectives
                and constraints.
            raster_table (RasterTable): class holding paths to rasters indexed
                by activity and factor names: raster_table[a][f] -> path.
                Has attributes activity_names and factor_names containing lists
                of each naming type taken from the raster table file.
            mask_raster (string): path to a single band raster that indicates
                pixels of interest.  This raster must be the same size and
                be in the same geospatial projection as those in
                `raster_list`.  Any nodata pixels in this list mask out all
                values given in `raster_list`.
            workspace (string): path to a directory to use as a workspace.
                NOT SURE WHERE THIS IS USED
            output_folder (string): path to output files. This will include
                at least the generated spatial decision unit grid. ANY OTHERS?
                NOTE IN EXAMPLE IT'S LISTED TWICE.
            grid_type (string): one of 'square' or 'hexagon' which is used to
                indicate the shape of the spatial decision grid generated by
                this worksflow.
            cell_size (int/float): indicates the cell size of the spatial
                decision unit grid in the projected linear units of the
                `mask_raster`.  If `grid_type` is 'square', then this
                indicates the side length of the grid.  If `hexagon`,
                indicates minor axis length.
            csv_output_folder (string): destination folder for the output csvs.
                Individual files will be named after their activity name.
            baseline_file (string): name of target baseline file which is a
                copy of `csv_filepath` except all marginal values are set to
                0.
            sdu_id_col (string): desired name of the column generated in
                `csv_filepath` which represents the spatial decision unit ID.

    Returns:
        None.
    """
    print('BEGINNING PREPROCESSING')

    # print('CHECK: Ensuring inputs in same projection and same size.')
    # _assert_inputs_same_size_and_srs(args['raster_list'] + [args['mask_raster']],
    #                                  args['serviceshed_list'])

    print('STEP: Constructing output paths')
    f_reg = {
        'sdu_grid': os.path.join(args['workspace'], 'sdu_grid.shp'),
        'table_folder': args['csv_output_folder'],
        'baseline_ip_table': os.path.join(
            args['csv_output_folder'], args['baseline_file']),
        }
    for path in f_reg.values():
        dir_path = os.path.dirname(path)
        if not os.path.exists(dir_path):
            os.makedirs(dir_path)

    # Create merged activity mask
    mask_path_list = list(args['activity_masks'].values())
    all_activity_mask = os.path.join(args['workspace'], 'all_activity_mask.tif')
    _create_overlapping_activity_mask(mask_path_list, all_activity_mask)

    # Creating or copying SDU shapefile
    if args['grid_type'] in ['square', 'hexagon']:
        print('STEP: Creating grid')
        _grid_vector_across_raster(
            all_activity_mask, args['grid_type'], args['cell_size'],
            f_reg['sdu_grid'], args['sdu_id_col'])
    elif os.path.isfile(args['grid_type']):
        _copy_sdu_file(args['grid_type'], f_reg['sdu_grid'])
    else:
        print('invalid SDU grid argument: {}'.format(args['grid_type']))
    # TODO: do we want to clean up the polygons either way? or only for a regular grid?
    if "filter_sdu" in args and args["filter_sdu"] is True:
        print('STEP: Removing polygons that only cover nodata')
        _remove_nonoverlapping_sdus(
            f_reg['sdu_grid'], all_activity_mask, args['sdu_id_col'])

    f_reg['sdu_raster'] = _rasterize_sdus(
        f_reg['sdu_grid'], args['sdu_id_col'], all_activity_mask, args['workspace'])

    # Calculate serviceshed coverage and average values
    print('STEP: Calculate SDU serviceshed coverage')
    if 'serviceshed_list' in args and args['serviceshed_list'] is not None:
        sdu_serviceshed_coverage = _serviceshed_coverage(
            f_reg['sdu_grid'], args['sdu_id_col'], args["serviceshed_list"],
            args["serviceshed_names"], args["serviceshed_values"])
    else:
        sdu_serviceshed_coverage = None

    # ACTIVITY-SPECIFIC PROCESSING
    raster_table = args['raster_table']
    activities = raster_table.activity_names
    
    # process baseline first if we have it
    if "baseline" in activities:
        activities = ["baseline"] + [a for a in activities if a != "baseline"]
    baseline_raster_file_lookup = None

    for activity in activities:
        print(f'aggregating rasters for {activity}')
        table_file = os.path.join(args['csv_output_folder'], activity+'.csv')
        mask_path = args['activity_masks'][activity]

        # raster cleaning
        print('INTERMEDIATE: cleaning marginal value rasters with large '
              'and corrupt negative nodata')
        #TODO: remove this part - this was just to fix a particular set of rasters
        # clean_raster_file_lookup = {}
        # print(f'raster_table[activity]: {raster_table[activity]}')
        # for mv_id, mv_path in raster_table[activity].items():
        #     f_reg[mv_id] = os.path.join(args['workspace'], os.path.basename(mv_path))
        #     _clean_negative_nodata_values(mv_path, f_reg[mv_id])
        #     clean_raster_file_lookup[mv_id] = f_reg[mv_id]
        # print(f'clean_raster_file_lookup: {clean_raster_file_lookup}')

        # if activity == "baseline":
        #     baseline_raster_file_lookup = clean_raster_file_lookup["baseline"]
        

        # # AGGREGATE THE RASTERS        
        # print('STEP: Aggregate Value Rasters to SDU')
        # marginal_value_lookup = _aggregate_raster_values(
        #     f_reg['sdu_raster'], f_reg['sdu_grid'], args['sdu_id_col'], mask_path, clean_raster_file_lookup,
        #     baseline_raster_lookup=baseline_raster_file_lookup)
        # if activity == "baseline":
        #     baseline_value_lookup = marginal_value_lookup

        # # UNPACK MARGINAL VALUE LOOKUP TO TABLE
        # print('STEP: Create IP table')
        # _build_ip_table(
        #     args['sdu_id_col'], activities, activity, marginal_value_lookup,
        #     sdu_serviceshed_coverage, table_file)

        # CREATE ACTIVITY TABLE
        _create_value_tables_for_activity(
            f_reg["sdu_raster"],
            raster_table[activity],
            activities,
            activity,
            args["csv_output_folder"],
            sdu_grid_path=f_reg["sdu_grid"],
            mask_raster_path=mask_path,
            calc_area_for_activity=activity
        )

        # ADD COMBINED FACTORS TO TABLE
        print('Step: Add combined factors')
        if 'combined_factors' in args and args['combined_factors'] is not None:
            _add_combined_factors(table_file, args['combined_factors'])

    # MAKE DUMMY BASELINE TABLE IF NEEDED
    if "baseline" not in activities:
        print('STEP: Create baseline table')
        baseline_value_lookup = marginal_value_lookup.copy()
        # zero out all the marginal values since that's equivalent of baseline
        for marginal_value_tuple in baseline_value_lookup.values():
            for mv_id in marginal_value_tuple[1]:
                marginal_value_tuple[1][mv_id] = [0.0, 0, 0.0]

        _build_ip_table(
            args['sdu_id_col'], activities, None, baseline_value_lookup, sdu_serviceshed_coverage,
            f_reg['baseline_ip_table'], baseline_table=True)
        if 'combined_factors' in args and args['combined_factors'] is not None:
            _add_combined_factors(f_reg['baseline_ip_table'], args['combined_factors'])


def _serviceshed_coverage(
        sdu_grid_path, sdu_id_fieldname, serviceshed_path_list,
        serviceshed_id_list, serviceshed_values):
    """Construct serviceshed SDU coverage lookup.

    Calculates sum of SDU coverage over all polygons in the serviceshed path.

    Parameters:
        sdu_grid_path (string): path to a single layer SDU polygon vector
            that has a key field id named `sdu_id_fieldname`.
        sdu_id_fieldname (string): fieldname in the `sdu_grid_path` that
            indicates the integer ID of each SDU.
        serviceshed_path_list (string): list of paths to serviceshed polygon
            vectors to aggregate over.
        serviceshed_id_list (string): parallel list to `serviceshed_path_list`
            indicating the desired ID strings to index serviceshed polygon
            overlap per SDU.
        serviceshed_values (dict): a dictionary indexed by serviceshed ids
            defined in serviceshed_id_list.  If present, value is a list of
            serviceshed features that should be multiplied by sdu/serviceshed
            overlap proportion, and summed in dictionary.  See return value
            for example.


    Returns:
        a dictionary of the form:
            {
                sdu_id_0: {
                    "serviceshed_id_a":
                        (serviceshed coverage proportion for a on id_0,
                         {service_shed_a_value_i: sum of value_i multiplied
                          by proportion of coverage of sdu_id_0 with
                          servicshed _id_a.})
                    "serviceshed_id_b": ....
                },
                sdu_id_1: {....
            }
    """
    sdu_vector = ogr.Open(sdu_grid_path)
    sdu_layer = sdu_vector.GetLayer()

    sdu_lookup = {}
    for sdu_feature in sdu_layer:
        sdu_lookup[sdu_feature.GetField(str(sdu_id_fieldname))] = (
            shapely.wkb.loads(bytes(sdu_feature.GetGeometryRef().ExportToWkb())))
    sdu_layer = None
    sdu_vector = None

    result = collections.defaultdict(
        lambda: collections.defaultdict(
            lambda: [0.0, collections.defaultdict(float)]))

    for serviceshed_id, serviceshed_path in zip(
            serviceshed_id_list, serviceshed_path_list):
        print('serviceshed_id: {}'.format(serviceshed_id))
        print('serviceshed_values[serviceshed_id]: {}'.format(serviceshed_values[serviceshed_id]))
        serviceshed_vector = ogr.Open(serviceshed_path)
        for serviceshed_layer in serviceshed_vector:
            for serviceshed_feature in serviceshed_layer:
                serviceshed_geometry = serviceshed_feature.GetGeometryRef()
                serviceshed_polygon = shapely.wkb.loads(
                    bytes(serviceshed_geometry.ExportToWkb()))
                prep_serviceshed_polygon = shapely.prepared.prep(
                    serviceshed_polygon)
                for sdu_id, sdu_poly in sdu_lookup.items():
                    if not prep_serviceshed_polygon.intersects(sdu_poly):
                        # define some placeholders just in case so when we
                        # create a flat table later, everything is defined
                        # everywhere.
                        if serviceshed_id not in result[sdu_id]:
                            result[sdu_id][serviceshed_id][0] = 0.0
                            if serviceshed_id in serviceshed_values:
                                for value_id in serviceshed_values[serviceshed_id]:
                                    result[sdu_id][serviceshed_id][1][value_id] = 0.0
                        continue

                    sdu_intersection = sdu_poly.intersection(
                        serviceshed_polygon)
                    coverage_proportion = (
                        sdu_intersection.area / sdu_poly.area)
                    result[sdu_id][serviceshed_id][0] += (
                        coverage_proportion)
                    if serviceshed_id in serviceshed_values:
                        for value_id in serviceshed_values[serviceshed_id]:
                            if serviceshed_feature.GetField(str(value_id)) is None:
                                raise ValueError('shapefile contains NULL or missing values\n\t{}\nin field {}'.format(
                                    serviceshed_path, str(value_id)
                                ))
                            result[sdu_id][serviceshed_id][1][value_id] += (
                                coverage_proportion *
                                serviceshed_feature.GetField(
                                    str(value_id)))
    return result


def _build_ip_table(
        sdu_col_name, activity_list, activity_name, marginal_value_lookup,
        sdu_serviceshed_coverage, target_ip_table_path, baseline_table=False):
    """Build a table for Integer Programmer.

    Output is a CSV table with columns identifying the aggregating SDU_ID,
    stats about SDU and mask coverage, as well as aggregate values for
    marginal values.

    Parameters:
        sdu_col_name (string): desired name of the SDU id column in the
            target IP table.
        marginal_value_lookup (dict): in pseudocode:
         { sdu_id0:
                (sdu area, sdu pixel coverage, mask pixel count,
                 mask pixel coverage in Ha),
                {marginal value id a: (
                    aggreated values, n pixels of coverage,
                    aggregated value per Ha of covrage),
                 marginal value id b: ...},
              sdu_id1: ...
            }
        sdu_serviceshed_coverage (dict): in pseudocode:
            {
                sdu_id_0: {
                    "serviceshed_id_a":
                        [serviceshed coverage proportion for a on id_0,
                         {service_shed_a_value_i: sum of value_i multiplied
                          by proportion of coverage of sdu_id_0 with
                          servicshed _id_a.}]
                    "serviceshed_id_b": ....
                },
                sdu_id_1: {....
            }
        target_ip_table_path (string): path to target IP table that will
            have the columns:
                SDU_ID,pixel_count,area_ha,maskpixct,maskpixha,mv_ida,mv_ida_perHA
    """
    if activity_name is not None:
        try:
            activity_index = activity_list.index(activity_name)
        except ValueError:
            msg = 'activity_name not found in activity_list in _build_ip_table'
            raise RootPreprocessingError(msg)
    else:
        activity_index = None

    with open(target_ip_table_path, 'w') as target_ip_file:
        # write header
        target_ip_file.write(
            "{},pixel_count,area_ha".format(sdu_col_name))
        target_ip_file.write(",%s_ha" * len(activity_list) % tuple(activity_list))
        target_ip_file.write(',exclude')
        # target_ip_file.write(
        #     "{},pixel_count,area_ha,{}_px,{}_ha".format(
        #         sdu_col_name, activity_name, activity_name))
        # This gets the "first" value in the dict, then the keys of that dict
        # also makes sense to sort them so it's easy to navigate the CSV.
        marginal_value_ids = sorted(
            list(marginal_value_lookup.values())[0][1].keys())
        n_mv_ids = len(marginal_value_ids)
        target_ip_file.write((",%s" * n_mv_ids) % tuple(marginal_value_ids))
        # target_ip_file.write(
        #     (",%s_perHA" * n_mv_ids) % tuple(marginal_value_ids))
        if sdu_serviceshed_coverage is not None:
            first_serviceshed_lookup = list(sdu_serviceshed_coverage.values())[0]
        else:
            first_serviceshed_lookup = {}
        serviceshed_ids = sorted(first_serviceshed_lookup.keys())
        target_ip_file.write(
            (",%s" * len(serviceshed_ids)) % tuple(serviceshed_ids))
        value_ids = {
            sid: sorted(first_serviceshed_lookup[sid][1].keys()) for
            sid in serviceshed_ids
            }
        for serviceshed_id in serviceshed_ids:
            for value_id in value_ids[serviceshed_id]:
                target_ip_file.write(",%s_%s" % (serviceshed_id, value_id))
        target_ip_file.write('\n')

        # write each row
        for sdu_id in sorted(marginal_value_lookup):
            # id, pixel count, total pixel area,
            target_ip_file.write(
                "%d,%d,%f" % (
                    sdu_id, marginal_value_lookup[sdu_id][0][1],
                    marginal_value_lookup[sdu_id][0][0]))

            # areas by activity
            areas = [0 for _ in range(len(activity_list))]
            if baseline_table is False and activity_index is not None:
                areas[activity_index] = marginal_value_lookup[sdu_id][0][3]
            target_ip_file.write(",%f" * len(areas) % tuple(areas))
            # if all areas are 0, that means in particular the current activity has 0 available area
            # and we want to exclude this SDU as an option
            if baseline_table is False and max(areas) == 0:
                target_ip_file.write(',1')
            else:
                target_ip_file.write(',0')

            # write out all the marginal value aggregate values
            for mv_id in marginal_value_ids:
                target_ip_file.write(
                    ",%f" % marginal_value_lookup[sdu_id][1][mv_id][0])
            # write out all marginal value aggregate values per Ha
            # for mv_id in marginal_value_ids:
            #     target_ip_file.write(
            #         ",%f" % marginal_value_lookup[sdu_id][1][mv_id][2])
            # serviceshed values
            for serviceshed_id in serviceshed_ids:
                target_ip_file.write(
                    (",%f" % sdu_serviceshed_coverage[sdu_id][serviceshed_id][0]))
            for serviceshed_id in serviceshed_ids:
                for value_id in value_ids[serviceshed_id]:
                    target_ip_file.write(
                        (",%f" % sdu_serviceshed_coverage[sdu_id][serviceshed_id][1][value_id]))
            target_ip_file.write('\n')


def _add_combined_factors(data_table_path, combined_factors_dict):
    """
    Opens data_table_path as dataframe, then for each name and list of factors in
    combined_factors_dict, creates a new column 'name' with the value multiplying
    the columns in the CF dict value.

    :param data_table_path:
    :param combined_factors_dict:
    :return:
    """
    df = pd.read_csv(data_table_path)

    for col_name, factors in combined_factors_dict.items():
        df[col_name] = ap.apply(df, factors)

    df.to_csv(data_table_path, index=False)


def _clean_negative_nodata_values(
        base_raster_path, target_clean_raster_path):
    """Reset large negative corrupt nodata values to valid ones.

    Parameters:
        base_raster_path (string): path to a single band floating point
            raster with a large negative nodata value and pixel values that
            might also  be nodata but are corrupt from roundoff error.
        target_clean_raster_path (string): path to desired target raster that
            will ensure the nodata value is a ffinto(float32).min and any
            values in the source raster that are close to that value are set
            to this.

    Returns:
        None.
    """
    target_nodata = np.finfo(np.float32).min
    raster_info = pygeoprocessing.get_raster_info(base_raster_path)
    base_nodata = raster_info['nodata'][0]
    if base_nodata > target_nodata / 10:
        print(
            "Base raster doesn't have a large nodata value; it's likely"
            " not one of the corrupt float32.min rasters we were dealing"
            " with.  Copying %s to %s without modification." % (
                base_raster_path, target_clean_raster_path))
        shutil.copy(base_raster_path, target_clean_raster_path)

    pygeoprocessing.new_raster_from_base(
        base_raster_path, target_clean_raster_path, gdal.GDT_Float32,
        [target_nodata])
    target_raster = gdal.Open(target_clean_raster_path, gdal.GA_Update)
    target_band = target_raster.GetRasterBand(1)

    for block_offset, data_block in pygeoprocessing.iterblocks(
            (base_raster_path, 1)):
        possible_nodata_mask = data_block < (target_nodata / 10)
        data_block[possible_nodata_mask] = target_nodata
        target_band.WriteArray(
            data_block, xoff=block_offset['xoff'], yoff=block_offset['yoff'])


def _aggregate_raster_values(
        sdu_raster_path, sdu_grid_path, sdu_key_id, mask_raster_path,
        value_raster_lookup, baseline_raster_lookup=None):
    """Build table that indexes SDU ids with aggregated marginal values.

    Parameters:
        sdu_grid_path (string): path to single layer polygon vector with
            integer field id that uniquely identifies each polygon.
        sdu_key_id (string): field in `sdu_grid_path` that uniquely identifies
            each feature.
        mask_raster_path (string): path to a mask raster whose pixels are
            considered "valid" if they are not nodata.
        value_raster_lookup (dict): keys are marginal value IDs that
            will be used in the optimization table; values are paths to
            single band rasters.
        baseline_raster_lookup (dict): same as `value_raster_lookup` but
            for the baseline rasters. Need this if calculating "merged" values.

    Returns:
        A dictionary that encapsulates stats about each polygon, mask coverage
        and marginal value aggregation and coverage. Each key in the dict is
        the SDU_ID for a polygon, while the value is a tuple that contains
        first polygon/mask stats, then another dict for marginal value stats.
        In pseudocode:
            { sdu_id0:
                (sdu area, sdu pixel coverage, mask pixel count,
                 mask pixel coverage in Ha),
                {marginal value id a: (
                    aggregated values, n pixels of coverage,
                    aggregated value per Ha of coverage),
                 marginal valud id b: ...},
              sdu_id1: ...
            }
    """
    # TODO: drop activity mask, get activity from the rasters - require nodata for non-transition pixels

    print('marginal_value_lookup: {}'.format(value_raster_lookup))
    value_ids = value_raster_lookup.keys()

    value_rasters = [
        gdal.Open(value_raster_lookup[value_id])
        for value_id in value_ids]
    value_bands = [
        raster.GetRasterBand(1) for raster in value_rasters]
    value_nodata_list = [
        band.GetNoDataValue() for band in value_bands]
    
    if baseline_raster_lookup is not None:
        baseline_value_rasters = [
            gdal.Open(baseline_raster_lookup[value_id])
            for value_id in value_ids]
        baseline_value_bands = [
            raster.GetRasterBand(1) for raster in baseline_value_rasters]
        baseline_value_nodata_list = [
            band.GetNoDataValue() for band in baseline_value_bands]

    mask_raster = gdal.Open(mask_raster_path)
    mask_band = mask_raster.GetRasterBand(1)
    mask_nodata = mask_band.GetNoDataValue()
    geotransform = mask_raster.GetGeoTransform()
    # note: i'm assuming square pixels that are aligned NS and EW and
    # projected in meters as linear units
    pixel_area_m2 = float((geotransform[1]) ** 2)

    id_raster = gdal.Open(sdu_raster_path)
    id_band = id_raster.GetRasterBand(1)
    id_nodata = id_band.GetNoDataValue()
    id_band = None
    id_raster = None

    # first element in tuple is the coverage stats:
    # (sdu area, sdu pixel count, mask pixel count, mask pixel coverage in Ha)
    # second element 3 element list (aggregate sum, pixel count, sum/Ha)
    value_sums = collections.defaultdict(
        lambda: (
            [0.0, 0, 0, 0.0],
            dict((mv_id, [0.0, 0, None]) for mv_id in value_ids)))

    # format of sdu_coverage is:
    # (sdu area, sdu pixel count, mask pixel count, mask pixel coverage in Ha)
    for block_offset, id_block in pygeoprocessing.iterblocks(
            (sdu_raster_path, 1)):
        value_blocks = [
            band.ReadAsArray(**block_offset) for band in value_bands]
        if baseline_raster_lookup is not None:
            baseline_value_blocks = [
                band.ReadAsArray(**block_offset) for band in baseline_value_bands]
        else:
            baseline_value_nodata_list = [None for _ in value_bands]
            baseline_value_blocks = [None for _ in value_bands]
        
        mask_block = mask_band.ReadAsArray(**block_offset)
        
        for aggregate_id in np.unique(id_block):
            if aggregate_id == id_nodata:
                continue
            aggregate_mask = id_block == aggregate_id
            # update sdu pixel coverage
            # value_sums[aggregate_id][0] =
            #    (sdu area, sdu pixel count, mask pixel count, mask pixel Ha)
            value_sums[aggregate_id][0][1] += np.count_nonzero(
                aggregate_mask)
            valid_mask_block = mask_block[aggregate_mask]
            value_sums[aggregate_id][0][2] += np.count_nonzero(
                valid_mask_block != mask_nodata)
            for mv_id, mv_nodata, mv_block, base_nodata, base_block in zip(
                    value_ids, value_nodata_list,
                    value_blocks, baseline_value_nodata_list, baseline_value_blocks):
                valid_mv_block = mv_block[aggregate_mask]
                # raw aggregation of marginal value
                # value_sums[aggregate_id][1][mv_id] =
                # (sum, pixel count, pixel Ha)
                value_sums[aggregate_id][1][mv_id][0] += np.nansum(
                    valid_mv_block[np.logical_and(
                        valid_mv_block != mv_nodata,
                        valid_mask_block != mask_nodata)])
                if base_block is not None:
                    value_sums[aggregate_id][1][mv_id][0] += np.nansum(
                        base_block[np.logical_and(
                            base_block != base_nodata,
                            valid_mask_block == mask_nodata)])

                # pixel count coverage of marginal value
                value_sums[aggregate_id][1][mv_id][1] += (
                    np.count_nonzero(np.logical_and(
                        valid_mv_block != mv_nodata,
                        valid_mask_block != mask_nodata)))
    
    # calculate SDU, mask coverage in Ha, and marginal value Ha coverage
    for sdu_id in value_sums:
        value_sums[sdu_id][0][0] = (
            value_sums[sdu_id][0][1] * pixel_area_m2 / 10000.0)
        value_sums[sdu_id][0][3] = (
            value_sums[sdu_id][0][2] * pixel_area_m2 / 10000.0)
        # calculate the 3rd tuple of marginal value per Ha
        for mv_id in value_sums[sdu_id][1]:
            if value_sums[sdu_id][1][mv_id][1] != 0:
                value_sums[sdu_id][1][mv_id][2] = (
                    value_sums[sdu_id][1][mv_id][0] / (
                        value_sums[sdu_id][1][mv_id][1] *
                        pixel_area_m2 / 10000.0))
            else:
                value_sums[sdu_id][1][mv_id][2] = 0.0
    
    del value_bands[:]
    del value_rasters[:]
    mask_band = None
    mask_raster = None
    return value_sums


def _grid_vector_across_raster(
        mask_raster_path, grid_type, cell_size, out_grid_vector_path,
        sdu_id_fieldname):
    """Convert vector to a regular grid.

    Here the vector is gridded such that all cells are contained within the
    original vector.  Cells that would intersect with the boundary are not
    produced.

    Parameters:
        mask_raster_path (string): path to a single band raster where
            pixels valued at '1' are valid and invalid otherwise.
        grid_type (string): one of "square" or "hexagon"
        cell_size (float): dimensions of the grid cell in the projected units
            of `vector_path`; if "square" then this indicates the side length,
            if "hexagon" indicates the width of the horizontal axis.
        out_grid_vector_path (string): path to the output ESRI shapefile
            vector that contains a gridded version of `vector_path`, this file
            should not exist before this call
        sdu_id_fieldname (string): desired key id field

    Returns:
        None
    """
    driver = ogr.GetDriverByName('ESRI Shapefile')
    if os.path.exists(out_grid_vector_path):
        driver.DeleteDataSource(out_grid_vector_path)

    raster_mask = gdal.Open(mask_raster_path)
    spatial_reference = osr.SpatialReference(raster_mask.GetProjection())

    out_grid_vector = driver.CreateDataSource(out_grid_vector_path)
    grid_layer = out_grid_vector.CreateLayer(
        'grid', spatial_reference, ogr.wkbPolygon)
    grid_layer.CreateField(
        ogr.FieldDefn(str(sdu_id_fieldname), ogr.OFTInteger))
    grid_layer_defn = grid_layer.GetLayerDefn()

    geotransform = raster_mask.GetGeoTransform()
    # minx maxx miny maxy
    extent = [
        geotransform[0],
        (geotransform[0] +
         raster_mask.RasterXSize * geotransform[1] +
         raster_mask.RasterYSize * geotransform[2]),
        (geotransform[3] +
         raster_mask.RasterXSize * geotransform[4] +
         raster_mask.RasterYSize * geotransform[5]),
        geotransform[3]
        ]
    raster_mask = None

    # flip around if one direciton is negative or not; annoying case that'll
    # always linger unless directly approached like this
    extent = [
        min(extent[0], extent[1]),
        max(extent[0], extent[1]),
        min(extent[2], extent[3]),
        max(extent[2], extent[3])]

    if grid_type == 'hexagon':
        # calculate the inner dimensions of the hexagons
        grid_width = extent[1] - extent[0]
        grid_height = extent[3] - extent[2]
        delta_short_x = cell_size * 0.25
        delta_long_x = cell_size * 0.5
        delta_y = cell_size * 0.25 * (3 ** 0.5)

        # Since the grid is hexagonal it's not obvious how many rows and
        # columns there should be just based on the number of squares that
        # could fit into it.  The solution is to calculate the width and
        # height of the largest row and column.
        n_cols = int(math.floor(grid_width / (3 * delta_long_x)) + 1)
        n_rows = int(math.floor(grid_height / delta_y) + 1)

        def _generate_polygon(col_index, row_index):
            """Generate a points for a closed hexagon."""
            if (row_index + 1) % 2:
                centroid = (
                    extent[0] + (delta_long_x * (1 + (3 * col_index))),
                    extent[2] + (delta_y * (row_index + 1)))
            else:
                centroid = (
                    extent[0] + (delta_long_x * (2.5 + (3 * col_index))),
                    extent[2] + (delta_y * (row_index + 1)))
            x_coordinate, y_coordinate = centroid
            hexagon = [(x_coordinate - delta_long_x, y_coordinate),
                       (x_coordinate - delta_short_x, y_coordinate + delta_y),
                       (x_coordinate + delta_short_x, y_coordinate + delta_y),
                       (x_coordinate + delta_long_x, y_coordinate),
                       (x_coordinate + delta_short_x, y_coordinate - delta_y),
                       (x_coordinate - delta_short_x, y_coordinate - delta_y),
                       (x_coordinate - delta_long_x, y_coordinate)]
            return hexagon
    elif grid_type == 'square':
        def _generate_polygon(col_index, row_index):
            """Generate points for a closed square."""
            square = [
                (extent[0] + col_index * cell_size + x,
                 extent[2] + row_index * cell_size + y)
                for x, y in [
                    (0, 0), (cell_size, 0), (cell_size, cell_size),
                    (0, cell_size), (0, 0)]]
            return square
        n_rows = int((extent[3] - extent[2]) / cell_size)
        n_cols = int((extent[1] - extent[0]) / cell_size)
    else:
        raise ValueError('Unknown polygon type: %s' % grid_type)

    for row_index in range(n_rows):
        for col_index in range(n_cols):
            polygon_points = _generate_polygon(col_index, row_index)
            ring = ogr.Geometry(ogr.wkbLinearRing)
            for xoff, yoff in polygon_points:
                ring.AddPoint(xoff, yoff)
            poly = ogr.Geometry(ogr.wkbPolygon)
            poly.AddGeometry(ring)

            poly_feature = ogr.Feature(grid_layer_defn)
            poly_feature.SetGeometry(poly)
            poly_feature.SetField(
                str(sdu_id_fieldname), row_index * n_cols + col_index)
            grid_layer.CreateFeature(poly_feature)
    grid_layer.SyncToDisk()


def _remove_nonoverlapping_sdus(vector_path, mask_raster_path, key_id_field):
    """Remove polygons in `vector_path` that don't overlap valid data.

    Parameters:
        vector_path (string): path to a single layer polygon shapefile
            that has a  unique key field named `key_id_field`.  This function
            modifies this polygon to remove any polygons.
        make_raster_path (string): path to a mask raster; polygons in
            `vector_path` that only overlap nodata pixels will be removed.
        key_id_field (string): name of key id field in the polygon vector.

    Returns:
        None.
    """
    with tempfile.NamedTemporaryFile(dir='.', delete=False) as id_raster_file:
        id_raster_path = id_raster_file.name

    pygeoprocessing.new_raster_from_base(
        mask_raster_path, id_raster_path, gdal.GDT_Int32,
        band_nodata_list=[-1], fill_value_list=[-1])

    id_raster = gdal.Open(id_raster_path, gdal.GA_Update)

    tmp_vector_dir = tempfile.mkdtemp()
    vector_basename = os.path.basename(vector_path)
    vector_driver = ogr.GetDriverByName("ESRI Shapefile")
    base_vector = ogr.Open(vector_path)
    vector = vector_driver.CopyDataSource(
        base_vector, os.path.join(tmp_vector_dir, vector_basename))
    base_vector = None
    layer = vector.GetLayer()

    gdal.RasterizeLayer(
        id_raster, [1], layer, options=['ATTRIBUTE=%s' % key_id_field])
    id_band = id_raster.GetRasterBand(1)
    mask_nodata = pygeoprocessing.get_raster_info(
        mask_raster_path)['nodata'][0]
    covered_ids = set()
    for mask_offset, mask_block in pygeoprocessing.iterblocks(
            (mask_raster_path, 1)):
        id_block = id_band.ReadAsArray(**mask_offset)
        valid_mask = mask_block != mask_nodata
        covered_ids.update(np.unique(id_block[valid_mask]))

    # cleanup the ID raster since we're done with it
    id_band = None
    id_raster = None
    os.remove(id_raster_path)

    # now it's sufficient to check if the min value on each feature is defined
    # if so there are valid pixels underneath, otherwise none.
    for feature in layer:
        feature_id = feature.GetField(str(key_id_field))
        if feature_id not in covered_ids:
            layer.DeleteFeature(feature.GetFID())

    print('INFO: Packing Target SDU Grid')
    # remove target vector and create a new one in its place with same layer
    # and fields
    os.remove(vector_path)
    target_vector = vector_driver.CreateDataSource(vector_path)
    spatial_ref = osr.SpatialReference(layer.GetSpatialRef().ExportToWkt())
    target_layer = target_vector.CreateLayer(
        str(os.path.splitext(vector_basename)[0]),
        spatial_ref, ogr.wkbPolygon)
    layer_defn = layer.GetLayerDefn()
    for index in range(layer_defn.GetFieldCount()):
        field_defn = layer_defn.GetFieldDefn(index)
        field_defn.SetWidth(24)
        target_layer.CreateField(field_defn)

    # copy over undeleted features
    layer.ResetReading()
    for feature in layer:
        target_layer.CreateFeature(feature)
    target_layer = None
    target_vector = None
    layer = None
    vector = None

    # remove unpacked vector
    shutil.rmtree(tmp_vector_dir)


def _rasterize_sdus(sdu_grid_path, sdu_key_id, reference_raster, workspace):
    # rasterize the SDU shapefile
    # with tempfile.NamedTemporaryFile(dir='.', delete=False) as id_raster_file:
    #     id_raster_path = id_raster_file.name
    id_raster_path = os.path.join(workspace, "sdu_grid.tif")
    id_nodata = -1
    pygeoprocessing.new_raster_from_base(
        reference_raster, id_raster_path,
        gdal.GDT_Int32, band_nodata_list=[id_nodata],
        fill_value_list=[id_nodata])

    vector = ogr.Open(sdu_grid_path, 1)  # open for reading
    layer = vector.GetLayer()
    id_raster = gdal.Open(id_raster_path, gdal.GA_Update)
    gdal.RasterizeLayer(
        id_raster, [1], layer, options=['ATTRIBUTE=%s' % sdu_key_id])
    id_raster = None
    return id_raster_path


def _assert_inputs_same_size_and_srs(raster_path_list, shapefile_path_list):
    """Assert all rasters in list have same dimensions and geoprojection.

    Raises a ValueError if any of the rasters in the list are of a different
    size or geospatial projection from each other.

    Parameter:
        raster_path_list (list): list of paths to rasters.

    Returns:
        None.
    """
    raster_list = [gdal.Open(raster_path) for raster_path in raster_path_list]
    raster_projections = []
    raster_geotransforms = []

    for raster in raster_list:
        projection_as_str = raster.GetProjection()
        raster_sr = osr.SpatialReference()
        raster_sr.ImportFromWkt(projection_as_str)
        raster_projections.append((raster_sr, raster.GetFileList()[0]))

        raster_geotransforms.append(raster.GetGeoTransform())

    for index in range(len(raster_projections)-1):
        if not raster_projections[index][0].IsSame(
                raster_projections[index+1][0]):
            raise ValueError(
                "These two rasters might not be in the same projection."
                " The different projections are:\n\n'filename: %s'\n%s\n\n"
                "and:\n\n'filename:%s'\n%s\n\n",
                raster_projections[index][1],
                raster_projections[index][0].ExportToPrettyWkt(),
                raster_projections[index+1][1],
                raster_projections[index+1][0].ExportToPrettyWkt())

        if not (raster_geotransforms[index][1] == raster_geotransforms[index+1][1] and
                raster_geotransforms[index][5] == raster_geotransforms[index+1][5]):
            raise ValueError("These rasters might not have the same cell size: \n\t{}\n\t{}".format(
                raster_path_list[index], raster_path_list[index+1]
            ))

        raster_spatial_extents_conditions = [
            raster_geotransforms[index][0] == raster_geotransforms[index + 1][0],
            raster_geotransforms[index][3] == raster_geotransforms[index + 1][3],
            raster_list[index].RasterXSize == raster_list[index + 1].RasterXSize,
            raster_list[index].RasterYSize == raster_list[index + 1].RasterYSize
        ]
        if not all(raster_spatial_extents_conditions):
            raise ValueError("These rasters might not have identical spatial extents {} : \n\t{}\n\t{}".format(
                raster_spatial_extents_conditions, raster_path_list[index], raster_path_list[index+1]
            ))

    shapefile_projections = []
    for shapefile in shapefile_path_list:
        driver = ogr.GetDriverByName('ESRI Shapefile')
        dataset = driver.Open(shapefile)
        layer = dataset.GetLayer()
        spatial_ref = layer.GetSpatialRef()

        vector_sr = osr.SpatialReference()
        vector_sr.ImportFromWkt(spatial_ref.ExportToWkt())
        shapefile_projections.append(vector_sr)

    raster_ref = raster_projections[0][0]
    for index in range(len(shapefile_projections)):
        if not raster_ref.IsSame(shapefile_projections[index]):
            raise ValueError(
                'The projection of this shapefile does not match raster projections: \n\t{}'.format(
                    shapefile_path_list[index]
                )
            )

    del raster_list[:]
    # del shapefile_list[:]


def _copy_sdu_file(source, dest):
    """
    Copies the shapefile, renaming it to sdu_grid.shp
    :param source:
    :param dest:
    :return:
    """
    shp_base = os.path.splitext(source)[0]
    shp_components = glob.glob(shp_base+'.*')
    dest_no_ext = os.path.splitext(dest)[0]
    for f in shp_components:
        ext = os.path.splitext(f)[1]
        new_f = os.path.join(dest_no_ext + ext)
        shutil.copy(f, new_f)


def _create_overlapping_activity_mask(mask_path_list,
                                      target_file,
                                      reference_mask=0):

    mask_nodatas = [
        pygeoprocessing.get_raster_info(raster_path)['nodata'][0] for
        raster_path in mask_path_list]

    def any_pixels_have_value(*vals):
        valid_pix_arrays = [x != nd for x, nd in zip(vals, mask_nodatas)]
        return np.any(valid_pix_arrays, axis=0)

    ref_info = pygeoprocessing.get_raster_info(mask_path_list[reference_mask])

    pygeoprocessing.raster_calculator(
        [(path, 1) for path in mask_path_list],
        any_pixels_have_value,
        target_file,
        ref_info['datatype'],
        ref_info['nodata'][0])


def _create_value_tables_for_activity(
    sdu_raster_path,
    value_raster_lookup, 
    activity_list,
    activity_name,
    target_folder,
    sdu_grid_path=None,
    sdu_list=None,
    mask_raster_path=None,
    fill_raster_lookup=None,
    calc_area_for_activity=None,
    sdu_id_column="SDU_ID",
    aoi_raster_path=None):
    """
    Reimplementation of `_aggregate_raster_values` and `_create_ip_table` that combines their functionality
    by skipping the intermediate data structure and adding the values directly into a pandas `DataFrame`.

    Here are the different cases we need to cover:
    1) Simple summation by SDU: this is the case for the baseline values, for example. We
       don't need to apply the mask, just sum the pixels in each of the value rasters
       according to the SDU mask.
    2) Masked sum: in this case, we only sum the values in pixels indicated by the mask raster.
       Other pixels are ignored. Do this if `mask_raster_path` is not `None` and 
       `nonmasked_raster_lookup` is `None.
    3) Masked sum with fill values: here we sum the values from `value_raster_lookup` where the
       mask indicates, and then sum values from `nonmasked_raster_lookup` elsewhere. In this case, 
       we do need to be careful to have appropriately masked to the AOI. 
    
    How these are used:
    * Absolute values. This mode optimizes based on the absolute values in the pixels and requires
      a baseline scenario. It will use case 1 to aggregate the baseline scenarios and case 3 to
      aggregate the specific activities.
    * Marginal values. This mode optimized based on pre-calculated marginals and has no baseline
      scenario. It will use case 2 to aggregate values.

    TODO: add serviceshed weights
    """

    if not os.path.isfile(sdu_raster_path):
        raise ValueError(f"sdu_raster_path {sdu_raster_path} not found")
    if sdu_list is None and sdu_grid_path is None:
        raise ValueError("One of sdu_list and sdu_grid_path must be provided")
    if sdu_grid_path is not None and not os.path.isfile(sdu_grid_path):
        raise ValueError(f"sdu_grid_path {sdu_grid_path} not found")

    # INITIALIZE THE DATAFRAME
    if sdu_list is not None:
        sdu_ids = sdu_list
    else:
        sdu_ids = _get_sdu_list(sdu_grid_path, sdu_id_column=sdu_id_column)
    value_ids = value_raster_lookup.keys()
    df = pd.DataFrame(index=sdu_ids)
    df.index.name = sdu_id_column

    columns = ["pixel_count", "area_ha", "exclude"]
    columns.extend([f"{activity}_ha" for activity in activity_list])
    columns.extend(value_ids)
    for col in columns:
        df[col] = 0
    
    # PREP THE RASTERS TO READ
    value_rasters = [
        gdal.Open(value_raster_lookup[value_id])
        for value_id in value_ids]
    value_bands = [
        raster.GetRasterBand(1) for raster in value_rasters]
    value_nodata_list = [
        band.GetNoDataValue() for band in value_bands]

    sdu_raster = gdal.Open(sdu_raster_path)
    sdu_band = sdu_raster.GetRasterBand(1)
    sdu_nodata = sdu_band.GetNoDataValue()
    geotransform = sdu_raster.GetGeoTransform()
    # note: i'm assuming square pixels that are aligned NS and EW and
    # projected in meters as linear units
    pixel_area_ha = float((geotransform[1]) ** 2) / 10000

    if mask_raster_path is not None:
        mask_raster = gdal.Open(mask_raster_path)
        mask_band = mask_raster.GetRasterBand(1)
        mask_nodata = mask_band.GetNoDataValue()

    if fill_raster_lookup is not None:
        fill_rasters = [gdal.Open(fill_raster_lookup[vid]) for vid in value_ids]
        fill_bands = [raster.GetRasterBand(1) for raster in fill_rasters]
        fill_nodata_values = [band.GetNoDataValue() for band in fill_bands]

    if aoi_raster_path is not None:
        aoi_raster = gdal.Open(aoi_raster_path)
        aoi_band = aoi_raster.GetRasterBand(1)
        aoi_nodata = aoi_band.GetNoDataValue()

    # LOOP THROUGH THE BLOCKS
    for block_offset, sdu_block in pygeoprocessing.iterblocks((sdu_raster_path, 1)):
        
        # load the value blocks and set to zeros in nodata pixels
        value_blocks = [band.ReadAsArray(**block_offset) for band in value_bands]
        for i, block in enumerate(value_blocks):
            block[block == value_nodata_list[i]] = 0
        
        if aoi_raster_path is not None:
        # If we have an AOI, all we need to do is remove non-AOI pixels from the SDU block,
        # since all the other summations are based on picking out the pixels that match the
        # SDU ID, ignoring any pixel where `sdu_block == sdu_nodata`. We don't need to
        # touch any of the value or fill rasters. 
            aoi_block = aoi_band.ReadAsArray(**block_offset)
            sdu_block[aoi_block == aoi_nodata] = sdu_nodata
        
        if mask_raster_path is not None:
            mask_block = mask_band.ReadAsArray(**block_offset)
        if fill_raster_lookup is not None:
            fill_blocks = [band.ReadAsArray(**block_offset) for band in fill_bands]
            for i, block in enumerate(fill_blocks):
                block[block == fill_nodata_values[i]] = 0

        # loop through each SDU ID that appears in this block
        for sdu in np.unique(sdu_block):
            if sdu == sdu_nodata:
                continue
            
            sdu_pix = sdu_block == sdu
            df.loc[sdu, "pixel_count"] += np.sum(sdu_pix)

            # CASE 1 - abs val mode: baseline scenario
            if mask_raster_path is None:
                if calc_area_for_activity is not None:
                    df.loc[sdu, f"{calc_area_for_activity}_ha"] += np.sum(sdu_pix) * pixel_area_ha
                for vid, vb in zip(value_ids, value_blocks):
                    df.loc[sdu, vid] += np.sum(vb[sdu_pix])
            # CASE 2 - marg val mode: activity scenarios
            elif mask_raster_path is not None and fill_raster_lookup is None:
                mask_pix = np.all([sdu_pix, mask_block != mask_nodata], axis=0)
                if calc_area_for_activity is not None:
                    df.loc[sdu, f"{calc_area_for_activity}_ha"] += np.sum(mask_pix) * pixel_area_ha
                for vid, vb in zip(value_ids, value_blocks):
                    df.loc[sdu, vid] += np.sum(vb[mask_pix])
            # CASE 3 - abs val mode: activity scenarios
            else:
                mask_pix = np.all([sdu_pix, mask_block != mask_nodata], axis=0)
                fill_pix = np.all([sdu_pix, mask_block == mask_nodata], axis=0)
                if calc_area_for_activity is not None:
                    df.loc[sdu, f"{calc_area_for_activity}_ha"] += np.sum(mask_pix) * pixel_area_ha
                for vid, vb, fb in zip(value_ids, value_blocks, fill_blocks):
                    df.loc[sdu, vid] += np.sum(vb[mask_pix])
                    df.loc[sdu, vid] += np.sum(fb[fill_pix])
            
    table_file = os.path.join(target_folder, f"{activity_name}.csv")
    df.to_csv(table_file)



def _get_sdu_list(sdu_grid_path, sdu_id_column="SDU_ID"):
    driver = ogr.GetDriverByName("ESRI Shapefile")
    vector = ogr.Open(sdu_grid_path)
    layer = vector.GetLayer()
    id_vals = sorted([feature[sdu_id_column] for feature in layer])
    layer = None
    vector = None
    return id_vals





if __name__ == '__main__':
    # LOOK HERE FOR AN EXAMPLE OF HOW TO RUN
    if len(sys.argv) == 2:
        JSON_ARGS_PATH = sys.argv[1]
    else:
        JSON_ARGS_PATH = (
            r"E:\Dropbox\optimization_data\banana_rich.json")
    execute(JSON_ARGS_PATH)
